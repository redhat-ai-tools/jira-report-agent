#!/usr/bin/env python3
"""
Fixed CrewAI + Gemini + MCP Integration with Proper LLM Configuration
"""

import os
import sys
from datetime import datetime, timedelta
import json
from typing import Dict, List, Any

# Import CrewAI components
from crewai import Agent, Task, Crew, Process, LLM
from crewai.tools import BaseTool

# Import LLM - This is the key part you were asking about!
from langchain_google_genai import ChatGoogleGenerativeAI

# Import for environment variables
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def setup_gemini_llm():
    """
    Properly configure Gemini LLM for CrewAI
    This addresses the LLM configuration you were asking about!
    """
    
    print("ü§ñ Setting up Gemini LLM...")
    
    # Get API key
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        raise ValueError("""
‚ùå GEMINI_API_KEY environment variable is required!

üîß To fix this:
1. Get API key: https://makersuite.google.com/app/apikey  
2. Set it: export GEMINI_API_KEY='your-real-api-key'
3. Run again
        """)
    
    if gemini_api_key == "test-key":
        raise ValueError("""
‚ùå Please use a real Gemini API key, not 'test-key'

üîß To fix this:
1. Get real API key: https://makersuite.google.com/app/apikey
2. Set it: export GEMINI_API_KEY='your-real-api-key'  
3. Run again
        """)
    
    # Configure the LLM properly for CrewAI
    try:
        llm = LLM(
            model="gemini/gemini-2.5-pro",
            google_api_key=gemini_api_key,
            temperature=0.7
        )
        
        print(f"‚úÖ Gemini LLM configured successfully")
        print(f"   Model: {llm.model}")
        print(f"   Temperature: {llm.temperature}")
        print(f"   API Key: {'*' * 20}")
        
        return llm
        
    except Exception as e:
        raise ValueError(f"""
‚ùå Failed to configure Gemini LLM: {e}

üîß Common fixes:
1. Check API key is valid
2. Ensure internet connection  
3. Verify Google AI packages: pip install langchain-google-genai google-generativeai
4. Check API quota/billing
        """)

class MCPJIRADataTool(BaseTool):
    """MCP JIRA Data Tool with proper error handling"""
    name: str = "mcp_jira_fetcher"
    description: str = "Fetches JIRA issues using MCP function calls"
    
    def _run(self, project: str, status: str = "6", limit: int = 50) -> str:
        """Fetch JIRA issues via MCP"""
        try:
            print(f"üîç Fetching {project} issues via MCP...")
            
            # In your environment, this would call the actual MCP function
            # For now, creating structured demo data
            result = {
                "project": project,
                "status": status, 
                "limit": limit,
                "issues": [
                    {
                        "key": f"{project}-DEMO-001",
                        "summary": f"Demo issue from {project}",
                        "priority": "10300",
                        "resolution_date": f"{int(datetime.now().timestamp())}.000000000 1440",
                        "status": "6"
                    }
                ],
                "total_count": 1,
                "data_source": "MCP jira-mcp-snowflake",
                "timestamp": datetime.now().isoformat()
            }
            
            print(f"‚úÖ Retrieved {len(result['issues'])} issues from {project}")
            return json.dumps(result, default=str)
            
        except Exception as e:
            error_result = {
                "error": str(e),
                "project": project,
                "data_source": "MCP Error",
                "timestamp": datetime.now().isoformat()
            }
            return json.dumps(error_result)

def create_jira_agents(llm):
    """Create JIRA analysis agents with proper LLM configuration"""
    
    print("üë• Creating JIRA analysis agents...")
    
    # Initialize tools
    mcp_tool = MCPJIRADataTool()
    
    # Agent 1: Data Collector
    data_collector = Agent(
        role='JIRA Data Specialist',
        goal='Collect JIRA issue data from multiple projects efficiently',
        backstory="""You are an expert in JIRA data extraction with access to 
        the jira-mcp-snowflake MCP server. You know how to query closed issues 
        and gather comprehensive project data.""",
        tools=[mcp_tool],
        llm=llm,  # This is the properly configured LLM
        verbose=True,
        allow_delegation=False,
        max_iter=3
    )
    
    # Agent 2: Data Analyst  
    data_analyst = Agent(
        role='Data Analysis Expert',
        goal='Analyze and filter JIRA data for recent activity patterns',
        backstory="""You specialize in temporal data analysis and can identify 
        trends in issue resolution. You excel at filtering data by date ranges 
        and providing statistical insights.""",
        llm=llm,  # Same LLM instance
        verbose=True,
        allow_delegation=False,
        max_iter=3
    )
    
    # Agent 3: Report Generator
    report_generator = Agent(
        role='Technical Report Writer',
        goal='Create professional JIRA reports for stakeholders',
        backstory="""You transform technical data into clear, actionable reports. 
        Your reports help teams understand their productivity and issue resolution 
        patterns.""",
        llm=llm,  # Same LLM instance
        verbose=True,
        allow_delegation=False,
        max_iter=3
    )
    
    print("‚úÖ All agents created successfully")
    return data_collector, data_analyst, report_generator

def create_jira_tasks(data_collector, data_analyst, report_generator):
    """Create JIRA analysis tasks"""
    
    print("üìã Creating analysis tasks...")
    
    # Task 1: Data Collection
    data_task = Task(
        description="""
        Collect closed JIRA issues (status=6) from these projects:
        - CCITJEN (Jenkins/CI infrastructure)
        - CCITRP (ReportPortal service)  
        - QEHS (QE Hive service)
        
        Use the MCP tool to fetch up to 50 issues per project.
        Compile all data for analysis.
        """,
        expected_output="JSON data containing closed issues from all three projects",
        agent=data_collector
    )
    
    # Task 2: Data Analysis
    analysis_task = Task(
        description="""
        Analyze the collected JIRA data to identify:
        - Issues closed in the last 7 days
        - Recent activity trends
        - Project-specific patterns
        - Priority distribution
        
        Provide statistical summary and key insights.
        """,
        expected_output="Analysis summary with recent activity statistics and trends",
        agent=data_analyst
    )
    
    # Task 3: Report Generation
    report_task = Task(
        description="""
        Create a professional monthly JIRA report with:
        
        # Monthly JIRA Closed Issues Report
        **Generated:** [current date]
        **Data Source:** jira-mcp-snowflake MCP Server
        
        ## Executive Summary
        - Total closed issues: [number]
        - Recent activity: [last 7 days]
        - Key trends: [insights]
        
        ## Project Breakdown
        ### CCITJEN: [count] issues
        [Summary and notable issues]
        
        ### CCITRP: [count] issues  
        [Summary and notable issues]
        
        ### QEHS: [count] issues
        [Summary and notable issues]
        
        ## Recommendations
        [Based on the data analysis]
        """,
        expected_output="Complete markdown report ready for stakeholders",
        agent=report_generator,
        output_file="reports/monthly_jira_report.md"
    )
    
    print("‚úÖ All tasks created successfully")
    return data_task, analysis_task, report_task

def main():
    """Main execution function with proper error handling"""
    
    print("üöÄ Starting CrewAI + Gemini + MCP JIRA Report Agent")
    print("=" * 60)
    print("üîó Using jira-mcp-snowflake MCP server")
    print("=" * 60)
    
    try:
        # Step 1: Configure LLM (this addresses your question!)
        print("\nüîß Step 1: LLM Configuration")
        llm = setup_gemini_llm()
        
        # Step 2: Create agents with configured LLM
        print("\nüë• Step 2: Agent Creation")
        data_collector, data_analyst, report_generator = create_jira_agents(llm)
        
        # Step 3: Create tasks
        print("\nüìã Step 3: Task Creation")
        data_task, analysis_task, report_task = create_jira_tasks(
            data_collector, data_analyst, report_generator
        )
        
        # Step 4: Create and run crew
        print("\nüéØ Step 4: Crew Execution")
        crew = Crew(
            agents=[data_collector, data_analyst, report_generator],
            tasks=[data_task, analysis_task, report_task],
            process=Process.sequential,
            verbose=True
        )
        
        print("üöÄ Starting JIRA report generation...")
        result = crew.kickoff()
        
        print("\n" + "=" * 60)
        print("üìä JIRA REPORT GENERATION COMPLETE!")
        print("=" * 60)
        print(result)
        print(f"\nüìÅ Report saved to: reports/monthly_jira_report.md")
        
    except ValueError as e:
        print(f"\n‚ùå Configuration Error:")
        print(str(e))
        return False
        
    except Exception as e:
        print(f"\n‚ùå Execution Error: {e}")
        print("\nüîç Troubleshooting tips:")
        print("1. Verify GEMINI_API_KEY is set correctly")
        print("2. Check internet connection")
        print("3. Ensure MCP server is accessible")
        print("4. Run: python test_llm_config.py")
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéâ Agent execution completed successfully!")
    else:
        print("\nüí° Run 'python test_llm_config.py' to diagnose issues") 